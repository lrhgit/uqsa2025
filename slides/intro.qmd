---
title: "Global Sensitivity Analysis for Cardiovascular Models"
subtitle: "Motivation, concepts, and intuition -- Seminar at PoliMi 3‚Äì4 February 2026" 
author: "Leif Rune Hellevik"
institute: "NTNU and Politecnico di Milano (PoliMi)"
---




## Welcome & framing

**Format**

* Short conceptual introductions
* Interactive notebook demonstrations
* Discussion-oriented, not tool-heavy
* Slides: https://lrhgit.github.io/uqsa2025/

::: notes
Welcome everyone and thank them for coming.

Briefly introduce yourself and your affiliation with NTNU and PoliMi LaBS.

Explain that this seminar is meant to be practical and concept-driven:
not a full course, not a software tutorial, but a guided exploration of
how global sensitivity analysis helps us reason about uncertainty in models.

Emphasize that notebooks are used as a medium for explanation and exploration,
not something participants are expected to run line-by-line during the seminar.

Set expectations for interaction and questions throughout the sessions.

Before we go any further, please open the seminar slides in your browser.
That way you can follow links and jump to notebooks when we get to the interactive parts.

:::


## What we will do today

- Revisit basic statistics
- Introduce variance-based SA
- Explore interactive notebooks



## From a VUCA world to uncertainty-aware modelling
::: {.incremental}
* Engineering decisions are increasingly made in a **VUCA context**  
  *(Volatility, Uncertainty, Complexity, Ambiguity)*  
* Models are essential tools for reasoning under uncertainty  
* However, models are:  
  * based on uncertain inputs  
  * built on simplifying assumptions  
  * often nonlinear and high-dimensional  
* **Uncertainty quantification (UQ)** provides a systematic framework 
  to represent, propagate, and interpret uncertainty in models
:::

::: notes
**VUCA** stands for **Volatility, Uncertainty, Complexity, and Ambiguity**.
It describes the kind of world where many engineering and decision problems exist today.

In a VUCA world, systems change over time, data are uncertain, and many factors interact in complex ways. Decisions often have to be made even when information is incomplete.

Models help us reason in this situation, but models are never perfect.
They are based on assumptions, simplified descriptions of reality, and uncertain inputs. This means model results are always uncertain.

**Uncertainty quantification (UQ)** helps us describe and propagate this uncertainty. It tells us *how uncertain* the model output is.

But, as emphasized by **Saltelli**, this is not enough for decision-making.
In practice, we also need to know **which uncertainties matter most**.

This is why **sensitivity analysis** is essential in a VUCA context:
it helps us understand which inputs drive the uncertainty in the results, and where modelling effort, data collection, or attention should be focused.
:::


## From uncertainty-aware models to credible evidence

::: {.incremental}
* Models increasingly support or replace experiments
  ‚Üí **in silico trials** and virtual cohorts

* In silico trials aim to:

  * reduce animal testing
  * limit human exposure
  * explore infeasible or rare scenarios

* **Key question:** when is a computational result credible - and why should we trust it?

* Credibility requires understanding uncertainty

  * not all uncertainties matter equally

* **Global sensitivity analysis (GSA)** operationalises credibility

  * connects uncertainty to decision relevance
  * identifies dominant sources of risk
  * informs where validation and modelling effort matter
:::

::: notes
One key reason why uncertainty and sensitivity analysis have become so important is the rise of **in silico trials**.

In many areas ‚Äî especially medical devices and biomechanics ‚Äî models increasingly support, or partially replace, physical experiments. This is driven by ethics, cost, time, and feasibility: we want to reduce animal testing, limit human exposure, and explore rare or infeasible scenarios.

This raises the key question on this slide: *when is a computational result credible ‚Äî and why should we trust it?*
Here, **credibility** is used in the ASME sense. A result is *credible* when there is sufficient evidence that the model is adequate for its **context of use**. **Trust** is the consequence ‚Äî we trust a result because we understand why it is credible.

Crucially, credibility is not about removing uncertainty everywhere. It is about understanding **which uncertainties matter for the decision**, and which do not. This point is central in ASME V&V and in Aldieri‚Äôs work on credibility assessment.

Uncertainty quantification tells us *how uncertain* a model result is.
Global sensitivity analysis tells us *why* it is uncertain ‚Äî by identifying the dominant sources of uncertainty and risk.

In this sense, **global sensitivity analysis operationalises credibility**: it links uncertainty to decision relevance and shows where evidence and modelling effort actually matter.
:::


## Digital twins need global sensitivity analysis

::: {.incremental}
* **Digital twins** rely on models for monitoring, prediction, and decision support

* They operate continuously under uncertainty

  * uncertain and evolving data
  * uncertain model parameters
  * changing system states

* Decisions must be made despite uncertainty

  * not all uncertainties can be reduced

* **Global sensitivity analysis (GSA)** supports credible digital twins

  * identifies decision-relevant uncertainties
  * assesses robustness to uncertainty
  * supports credibility for the intended context of use

* Without GSA, digital twins risk being precise but **not reliable**

  * apparent accuracy may hide sensitivity to decision-relevant uncertain inputs

:::

::: notes
Digital twins are a useful example of why credibility under uncertainty really matters.

A digital twin is a model used continuously to monitor, predict, or support decisions about a real system. Because of this, it operates under persistent uncertainty: data are noisy or incomplete, parameters are uncertain, and the system itself may evolve over time.

At the same time, decisions still have to be made. We cannot remove all uncertainty, so the key question again becomes: *which uncertainties matter for the decisions the digital twin is used to support?*

In the ASME sense, a digital twin is credible only if its results are robust for its **intended context of use**. Apparent precision is not enough.

Global sensitivity analysis provides this link. It identifies which uncertainties are decision-relevant and where the model is sensitive or fragile. In that way, GSA supports credibility rather than just numerical accuracy.

This example sets the stage for the more general question: what problem sensitivity analysis actually solves.

Here, *precise* means producing sharp, detailed outputs, often with many digits.
*Reliable* means that those outputs are robust to uncertainty in data, parameters, and assumptions.

Without GSA, a digital twin may look accurate, while being highly sensitive to **decision-relevant uncertainties**. Apparent precision can therefore be misleading.

Transition to next slide: 
Digital twins are just one example. The underlying issue is much more general.
In any complex model used for decisions, we need to know which uncertainties actually matter.

:::



## What problem sensitivity analysis solves?

* **Models are used under uncertainty**

  * Inputs are not known exactly

* **Complexity defeats intuition**

  * Nonlinearity and interactions matter

* **Decisions require prioritisation**

  * Not all uncertainties are equally important

* **Sensitivity analysis = relevance**

  * Identifies what really drives uncertainty


::: notes
We have already seen the first two points on earlier slides.

We use models under uncertainty: inputs are not known exactly. And as
models become nonlinear and high-dimensional, our intuition is no
longer reliable. These points are important, but they are not very
controversial.

The real motivation for sensitivity analysis lies in the last two
points.

Decisions require prioritisation. In practice, we cannot reduce all
uncertainties. We do not have unlimited time, data, or resources, so
we must decide where effort actually matters.

This is why sensitivity analysis is fundamentally about **relevance**.
It tells us which uncertainties really drive the uncertainty in the
model output, and which ones matter less for the question we are
asking.

In other words, sensitivity analysis is not about explaining every
detail of a model. It is about identifying what matters for decisions.
:::


## Main objectives of sensitivity analysis

::: {.incremental}
- **Factor prioritisation**  
  Identify which uncertain inputs contribute most to output uncertainty
- **Factor fixing (screening)**  
  Identify non-influential inputs that can be fixed without affecting results
- **Understanding model behaviour**  
  Reveal nonlinear effects and interactions between inputs
- **Supporting robust decisions**  
  Focus modelling, data collection, and calibration effort where it matters
:::

::: notes
Now that the problem is clear, let me be explicit about what
sensitivity analysis is actually used for.

The first objective is **factor prioritisation**.  Among all uncertain
inputs, which ones contribute most to the uncertainty in the model
output? These are the parameters that deserve attention, better data,
or careful calibration.

The second objective is **factor fixing**, also called screening.
Here we identify inputs that have little influence on the output. If
an input is not important, it can be fixed to a constant value without
affecting the results, which simplifies the model.

The third objective is **understanding model behaviour**.  Many models
are nonlinear and include interactions between inputs. Sensitivity
analysis helps reveal these effects and gives insight into how the
model actually responds to uncertainty.

The final objective is **supporting robust decisions**.  Instead of
trying to improve everything, sensitivity analysis helps focus
modelling, data collection, and validation effort where it matters
most.

These objectives will guide the rest of the seminar. Different
sensitivity analysis methods support different objectives, and there
is no single ‚Äúbest‚Äù method for all purposes.
:::


## From local to global sensitivity analysis

- Local (derivative-based) measures
- One-factor-at-a-time (OAT)
- Why they fail for nonlinear models

::: notes
The key message of this slide is very simple:

**Local sensitivity looks at the model in one point.
Global sensitivity looks at the model over the whole input space.**

Local sensitivity methods use derivatives or small changes around a single reference point.
They can be useful when the model is almost linear and uncertainty is very small.

But in most real problems:

* parameters are uncertain over wide ranges
* models are nonlinear
* parameters interact with each other

In these cases, local methods can be misleading.
They may say an input is unimportant just because the derivative is small at that one point.

Global sensitivity analysis takes uncertainty seriously.
Inputs are varied over their full ranges, and we study how they affect the output variability.

So the important shift here is:
we move from **‚ÄúWhat happens if I change one parameter a little bit?‚Äù**
to **‚ÄúWhich uncertainties matter most for the overall behaviour of the model?‚Äù**
:::


## Variance as a measure of importance

- Output variability as information
- Conditional expectation and variance
- Total variance decomposition

::: notes
Variance is fundamental in global sensitivity analysis because it
measures **uncertainty in the model output**.

If a model output has large variance, it means the prediction is uncertain.
If the variance is small, the prediction is more stable.
So variance gives us a natural way to quantify how uncertain the model result is.

The key idea in sensitivity analysis is to ask:
*How much of this output variance is caused by each input?*

This is why variance is so useful.  It can be **decomposed** into
contributions from individual inputs and from their interactions.
This is a central idea in Saltelli‚Äôs work.

Conditional expectation and conditional variance help us formalize this.
If fixing one input significantly reduces the output variance, then that input is important.
If fixing it changes little, then it is not very important.

So variance is not just a statistical choice.
It directly connects uncertainty in the inputs to uncertainty in the output,
and it allows us to rank inputs by how much they contribute to that uncertainty.

This is what makes variance the natural foundation for variance-based
sensitivity measures, such as Sobol indices.

So once we agree that variance measures output uncertainty, the next
step is to ask how this variance can be attributed to individual
inputs  this is exactly what Sobol indices do.

:::

## Sobol indices ‚Äì idea, not formulas

- First-order effects
- Total effects
- Interaction effects

::: notes

Sobol indices are the most common variance-based sensitivity measures.

The important point is not the formulas, but the **idea** behind them.

The **first-order Sobol index** measures how much of the output variance is explained by one input *alone*, on average.
If this value is large, the input has a strong direct effect on the output.

The **total Sobol index** measures the overall effect of an input.
This includes both its direct effect and all interactions with other inputs.
If the total index is much larger than the first-order index, it means interactions are important.

Sobol indices also allow us to quantify **interaction effects** explicitly.
This is something local or one-factor-at-a-time methods cannot capture.

So Sobol indices give us a clear and interpretable answer to the key question:
which inputs, alone or in interaction, drive the uncertainty in the model output?

This is why they are so central in global sensitivity analysis, especially in complex and nonlinear models.

:::


## seminar structure

The seminar is organised around three main components:

- motivating the usefulness of Sobol sensitivity indices for model-based decision making,
- showing how Monte Carlo sampling and polynomial chaos expansion (PCE) can be used to compute them,
- demonstrating the methods on simple examples (benchmark functions and wall models), supported by interactive notebook-based exploration and discussion.

The detailed and up-to-date agenda is available here:

üëâ **https://lrhgit.github.io/uqsa2025/seminar/agenda.html**

## notebooks and colab

Jupyter notebooks are interactive documents that combine

- short explanations (text and equations),
- executable code,
- figures and tables.

In this seminar, notebooks are used as a medium for explanation and exploration ‚Äî not as a software tutorial.

Google Colab lets you run the notebooks in a browser, without local installation:

- üëâ [open the notebook via the Colab-link](https://lrhgit.github.io/uqsa2025/)
- or navigate to https://lrhgit.github.io/uqsa2025/

::: notes
    If you‚Äôre following along in the slides, click the link to the notebook index now‚Ä¶
:::

## how to use the notebooks in this seminar

To make a notebook your own in Colab:

- File ‚Üí Save a copy in Drive
- add notes in text cells (your interpretation, reminders, questions),
- change parameters (including sliders) to explore ‚Äúwhat if?‚Äù scenarios,
- run selected cells to reproduce key results.

You do not need to run everything line-by-line during the sessions.

The goal is to build intuition: how uncertainty propagates and why Sobol indices change.

## colab workflow (practical notes)

When opening a notebook in Colab:

- use **File ‚Üí Save a copy in Drive** before making changes,
- if prompted, choose **Runtime ‚Üí Run all** to initialise the notebook,
- re-run a cell if you change parameters or sliders above it.

If something breaks, simply reload the page and start from your saved copy.

These notebooks are meant to be exploratory and robust ‚Äî not fragile.




## What notebooks will show

* **Scatterplots and Sobol indices**
  Complementary ways to understand sensitivity

* **How to compute Sobol indices with the Monte Carlo Method and Polynomial Chaos Expansions **


* **Simple linear model (Saltelli)**
  Build intuition in the simplest case

* **Benchmark nonlinear models**
  G-function and maybe other model functions

* **Applied example**
  Wall model for blood flow simulation

::: notes
The purpose of the notebooks is to show how the concepts we discussed work in practice.

We will mainly use **scatterplots** and **Sobol sensitivity indices** as two complementary tools.
Scatterplots help us see input‚Äìoutput relationships directly,
while Sobol indices give a quantitative measure of how important each input is for output uncertainty.

We start with a **simple linear model**, as used in the book by Saltelli.
This is the simplest case and helps build intuition about variance and sensitivity.

Then we move to standard **benchmark models**:
the G-function and the Ishigami function.
These models are nonlinear and include strong interaction effects,
so they clearly show why global sensitivity analysis is needed.

Finally, we look at an **applied example**:
a wall model for blood flow simulation.
This shows how the same ideas apply in a realistic engineering context.

In the notebooks, we will compute sensitivity indices using both
**Monte Carlo sampling** and **polynomial chaos expansion (PCE)**.
Both methods estimate the same quantities:
the output variance and the Sobol indices.
The difference is efficiency, not interpretation.

The goal is not to teach software or implementation details.
The goal is to build intuition about uncertainty propagation
and about how sensitivity analysis supports modelling and decision-making.
:::




## Selected references

**Foundations of global sensitivity analysis**
¬†¬†*Global Sensitivity Analysis: The Primer.* Saltelli et al., Wiley, 2008.
¬†¬†*Sensitivity estimates for nonlinear mathematical models.* Sobol‚Äô, I. M., Math. Model. Comput. Exp., 1993.

<br>

**Variance-based methods and practice**
¬†¬†*Variance-based sensitivity analysis of model output: Design and estimator for the total sensitivity index.*
Saltelli et al., Comput. Phys. Commun., 2010.

<br>

**Credibility, uncertainty, and modelling practice**
¬†¬†*Why so many published sensitivity analyses are false.* Saltelli et al., Environ. Model. Softw., 2019.
¬†¬†*Model credibility assessment and uncertainty quantification in in silico trials.* Aldieri et al. (ASME V&V).

<br>

**General overview and terminology**
¬†¬†*Sensitivity analysis.* Wikipedia ‚Äî [https://en.wikipedia.org/wiki/Sensitivity_analysis](https://en.wikipedia.org/wiki/Sensitivity_analysis)


Full reference list:  <a href="../seminar/references.html" target="_blank" rel="noopener">Seminar references </a>
