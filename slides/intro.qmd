---
title: "Global sensitivity analysis for cardiovascular models"
subtitle: "Motivation, concepts, and intuition -- Seminar at PoliMi 3‚Äì4 February 2026" 
author: "Leif Rune Hellevik"
institute: "NTNU and Politecnico di Milano (PoliMi)"
---




## Welcome & framing

**Format**

* Short conceptual introductions
* Interactive notebook demonstrations
* Discussion-oriented, not tool-heavy
* Slides: https://lrhgit.github.io/uqsa2025/

::: notes
Welcome everyone, both students and faculty in Gliwice Poland, my postdoc Rahul in Trondheim,
and to the group here at PoliMi (students, postdocs, and faculty), and in particular
Francesco who is hosting me.

and I am Leif Rune Hellevik, professor in biomechanics at NTNU, Trondheim
and currently at PoliMi at a sabbatical. 

This seminar on GSA for cardiovascular models is meant to be practical
and concept-driven: not a full course, not a software tutorial, but a
guided exploration of how GSA helps us reason about uncertainty in models.

Emphasize: notebooks are used as a medium for explanation and exploration,
not something participants are expected to run line-by-line during the seminar.

For you as attendants I encourage and hope for interaction and
questions throughout the sessions.

Before we go any further, please open the seminar slides in your browser.
https://lrhgit.github.io/uqsa2025/

That way you can follow links and jump to notebooks when we get to the interactive parts.

:::


## What we will do today

- Frame and motivate the need for GSA
- Revisit basic statistics
- Introduce variance-based SA
- Explore interactive notebooks



## From a VUCA world to uncertainty-aware modelling
::: {.incremental}

* VUCA (1990s ‚Üí today): a world of Volatility, Uncertainty, Complexity, and Ambiguity where deterministic reasoning is insufficient
* Implication for modelling: robust decisions require both uncertainty quantification (UQ) and sensitivity analysis (SA)
* Models are essential tools for reasoning under uncertainty  
* However, models are:  
  * based on uncertain inputs  
  * built on simplifying assumptions  
  * often nonlinear and high-dimensional  
* **Uncertainty quantification (UQ)** provides a systematic framework 
  to represent, propagate, and interpret uncertainty in models
:::

::: notes
**note to bullet 1 ‚Äî vuca**

VUCA was introduced in the early 1990s to describe the less
predictable post‚ÄìCold War world. Today it captures key features of
many engineering contexts: rapid change (volatility), imperfect
information (uncertainty), interacting components (complexity), and
multiple possible interpretations (ambiguity). In such settings,
purely deterministic reasoning is often insufficient.

**note to bullet 2 ‚Äî implication for modelling**

Models are essential in a VUCA world, but they are imperfect and
depend on uncertain inputs and simplifying assumptions. Uncertainty
quantification (UQ) tells us how uncertain model outputs are, while
sensitivity analysis (SA) tells us which uncertainties matter
most‚Äîmaking both crucial for robust decisions.

Models help us reason in this situation, but models are never perfect.
They are based on assumptions, simplified descriptions of reality, and
uncertain inputs. This means model results are always uncertain.

**Uncertainty quantification (UQ)** helps us describe and propagate
  this uncertainty. It tells us *how uncertain* the model output is.

But, as emphasized by **Saltelli**, this is not enough for decision-making.
In practice, we also need to know **which uncertainties matter most**.

This is why **sensitivity analysis** is essential in a VUCA context:
it helps us understand which inputs drive the uncertainty in the
results, and where modelling effort, data collection, or attention
should be focused.
:::


## From uncertainty-aware models to credible evidence

::: {.incremental}
* Models increasingly support or replace experiments
  ‚Üí **in silico trials** and virtual cohorts

* In silico trials aim to:

  * reduce animal testing
  * limit human exposure
  * explore infeasible or rare scenarios

* **Key question:** when is a computational result credible - and why should we trust it?

* Credibility requires understanding uncertainty

  * not all uncertainties matter equally

* **Global sensitivity analysis (GSA)** operationalises credibility

  * connects uncertainty to decision relevance
  * identifies dominant sources of risk
  * informs where validation and modelling effort matter
:::

::: notes

An important reason why UQSA analysis have
become so important is the rise of **in silico trials**.

In many areas ‚Äî especially medical devices and biomechanics ‚Äî models
increasingly support, or partially replace, physical experiments. This
is driven by ethics, cost, time, and feasibility: we want to reduce
animal testing, limit human exposure, and explore rare or infeasible
scenarios.

**Key question** This raises the key question on this slide: *when is a computational
result credible ‚Äî and why should we trust it?* Here, **credibility**
is used in the ASME sense. A result is *credible* when there is
sufficient evidence that the model is adequate for its **context of
use**. **Trust** is the consequence ‚Äî we trust a result because we
understand why it is credible.

Crucially, credibility is not about removing uncertainty
everywhere.

It is about understanding **which uncertainties matter for
the decision**, and which do not.
This point is central in ASME V&V
and in Aldieri‚Äôs work on credibility assessment.

UQ tells us *how uncertain* a model result is.
GSA tells us *why* it is uncertain ‚Äî by
identifying the dominant sources of uncertainty and risk.

In this sense, **GSA operationalises credibility**:
it links uncertainty to decision relevance and shows
where evidence and modelling effort actually matter.

:::


## Digital twins need global sensitivity analysis (GSA)

::: {.incremental}
* **DT** rely on models for monitoring, prediction, feedback and decision support

* They operate continuously under uncertainty

  * uncertain and evolving data
  * uncertain model parameters
  * changing system states

* Decisions must be made despite uncertainty

  * not all uncertainties can be reduced

* **GSA** supports credible digital twins

  * identifies decision-relevant uncertainties
  * assesses robustness to uncertainty
  * supports credibility for the intended context of use

* Without GSA, digital twins risk being precise but **not reliable**

  * apparent accuracy may hide sensitivity to decision-relevant uncertain inputs

:::

::: notes

Digital twins are a good example of why credibility under uncertainty
matters.

A digital twin is a model that is used continuously to monitor a
system, make predictions, and support decisions. Because of this, it
always operates under uncertainty ‚Äî in the data, in the model
parameters, and because the real system itself may change over time.

At the same time, decisions still have to be made. We cannot remove
all uncertainty, so the key question becomes: which uncertainties
actually matter for the decisions we care about?

In the ASME sense, a digital twin is credible only if it is robust for
its intended context of use ‚Äî that is, if it is *fit for
purpose*. Looking precise is not enough.

This is where global sensitivity analysis (GSA) comes in. GSA helps us
identify the uncertainties that are decision-relevant and where the
model is fragile. In this way, GSA supports credibility, not just
numerical accuracy.

So, without GSA, a digital twin may look very precise but still be
unreliable.

Transition: Digital twins are just one example ‚Äî this issue applies to
any complex model used for decision-making.

:::



## What problem sensitivity analysis solves?

* **Models are used under uncertainty**

  * Inputs are not known exactly

* **Complexity defeats intuition**

  * Nonlinearity and interactions matter

* **Decisions require prioritisation**

  * Not all uncertainties are equally important

* **Sensitivity analysis = relevance**

  * Identifies what really drives uncertainty


::: notes
We‚Äôve already touched on the first two points before.

We use models under uncertainty ‚Äî inputs are never known exactly. And
as models become nonlinear and high-dimensional, our intuition often
fails. These points are important, but not very controversial.

The real motivation for sensitivity analysis is in the last two points.

In practice, decisions require prioritisation. We don‚Äôt have unlimited
time, data, or resources, so we cannot reduce all uncertainties. We
have to focus our effort where it actually matters.

That is why sensitivity analysis is essentially about relevance. It
helps us identify which uncertainties truly drive the uncertainty in
the model output, and which ones matter less for the decision at hand.

So, sensitivity analysis is not about explaining every detail of a
model ‚Äî it is about understanding what matters for decision-making.

:::


## Main objectives of sensitivity analysis

::: {.incremental}
- **Factor prioritisation**  
  Identify which uncertain inputs contribute most to output uncertainty
- **Factor fixing (screening)**  
  Identify non-influential inputs that can be fixed without affecting results
- **Understanding model behaviour**  
  Reveal nonlinear effects and interactions between inputs
- **Supporting robust decisions**  
  Focus modelling, data collection, and calibration effort where it matters
:::

::: notes

Now that we know the problem, let me briefly summarise what
sensitivity analysis is actually used for.

First, factor prioritisation. We want to find which uncertain inputs
matter most for the output. These are the parameters that deserve
better data, calibration, or closer attention.

Second, factor fixing, or screening. Here we look for inputs that have
little influence. If an input is unimportant, we can fix it to a
constant value and simplify the model without changing the results.

Third, understanding model behaviour. Many models are nonlinear and
include interactions between inputs. Sensitivity analysis helps reveal
these effects and gives insight into how the model really behaves.

Finally, supporting robust decisions. Instead of trying to improve
everything, sensitivity analysis helps us focus modelling, data
collection, and validation effort where it actually matters.

These objectives will guide the rest of the seminar ‚Äî different
sensitivity analysis methods serve different purposes, and there is no
single ‚Äúbest‚Äù method for all problems.

:::


## From local to global sensitivity analysis

- Local (derivative-based) measures
- One-factor-at-a-time (OAT)
- Why they fail for nonlinear models

::: notes
The key message of this slide is very simple:

**Local sensitivity looks at the model in one point.
Global sensitivity looks at the model over the whole input space.**

Local sensitivity methods use derivatives or small changes around a
single reference point.  They can be useful when the model is almost
linear and uncertainty is very small.

But in most real problems:

* parameters are uncertain over wide ranges
* models are nonlinear
* parameters interact with each other

In these cases, local methods can be misleading.  They may say an
input is unimportant just because the derivative is small at that one
point.

Global sensitivity analysis takes uncertainty seriously.  Inputs are
varied over their full ranges, and we study how they affect the output
variability.

So the important shift here is:
we move from **‚ÄúWhat happens if I change one parameter a little bit?‚Äù**
to **‚ÄúWhich uncertainties matter most for the overall behaviour of the model?‚Äù**
:::


## Variance as a measure of importance

- Output variability as information
- Conditional expectation and variance
- Total variance decomposition

::: notes

Variance is central in GSA because it measures uncertainty in the model output.

If the output variance is large, the prediction is uncertain. If it is
small, the prediction is more stable. So variance gives us a clear way
to quantify how uncertain our model results are.

The key question in sensitivity analysis is:
How much of this output variance comes from each input?

This is why variance is so useful ‚Äî it can be decomposed into
contributions from individual inputs and from their interactions,
which is a core idea in Saltelli‚Äôs work.

Conditional expectation and conditional variance help formalise
this. If fixing one input greatly reduces the output variance, that
input is important. If it makes little difference, the input is less
important.

So variance is not just a statistical choice ‚Äî it directly links
uncertainty in the inputs to uncertainty in the output, and allows us
to rank inputs by importance.

This provides the foundation for variance-based measures like Sobol
indices, which we will look at next.


:::

## Sobol indices ‚Äì idea, not formulas

- First-order effects
- Total effects
- Interaction effects

::: notes

Sobol indices are the most widely used variance-based sensitivity
measures.

The key point is not the formulas, but the idea behind them.

The first-order Sobol index tells us how much of the output variance
is explained by one input alone. A large value means that input has a
strong direct effect.

The total Sobol index measures the overall effect of an input ‚Äî both
its direct effect and all its interactions with other inputs. If the
total index is much larger than the first-order index, interactions
are important.

Sobol indices also let us quantify interaction effects, which
one-at-a-time or purely local methods cannot do.

In short, Sobol indices answer the central question in sensitivity
analysis: which inputs ‚Äî alone or in combination ‚Äî really drive the
uncertainty in the model output?

:::


## Seminar structure

The seminar is organised around three main components:

- motivating the usefulness of Sobol sensitivity indices for model-based decision making,
- showing how Monte Carlo sampling and polynomial chaos expansion (PCE) can be used to compute them,
- demonstrating the methods on simple examples (benchmark functions and wall models), supported by interactive notebook-based exploration and discussion.

The detailed and up-to-date agenda is available here:

üëâ **https://lrhgit.github.io/uqsa2025/seminar/agenda.html**


## Notebooks and colab

Jupyter notebooks are interactive documents that combine

- short explanations (text and equations),
- executable code,
- figures and tables.

In this seminar, notebooks are used as a medium for explanation and exploration ‚Äî not as a software tutorial.

Google Colab lets you run the notebooks in a browser, without local installation:

- üëâ [open the notebook via the Colab-link](https://lrhgit.github.io/uqsa2025/)
- or navigate to https://lrhgit.github.io/uqsa2025/

::: notes
    If you‚Äôre following along in the slides, click the link to the notebook index now‚Ä¶
:::

## How to use the notebooks in this seminar

To make a notebook your own in Colab:

- File ‚Üí Save a copy in Drive
- add notes in text cells (your interpretation, reminders, questions),
- change parameters (including sliders) to explore ‚Äúwhat if?‚Äù scenarios,
- run selected cells to reproduce key results.

You do not need to run everything line-by-line during the sessions.

The goal is to build intuition: how uncertainty propagates and why Sobol indices change.

## Colab workflow (practical notes)

When opening a notebook in Colab:

- use **File ‚Üí Save a copy in Drive** before making changes,
- if prompted, choose **Runtime ‚Üí Run all** to initialise the notebook,
- re-run a cell if you change parameters or sliders above it.

If something breaks, simply reload the page and start from your saved copy.

These notebooks are meant to be exploratory and robust ‚Äî not fragile.




## What notebooks will show

* **Scatterplots and Sobol indices**
  Complementary ways to understand sensitivity

* **How to compute Sobol indices with the Monte Carlo Method and Polynomial Chaos Expansions **


* **Simple linear model (Saltelli)**
  Build intuition in the simplest case

* **Benchmark nonlinear models**
  G-function and maybe other model functions

* **Applied example**
  Wall model for blood flow simulation

::: notes
The purpose of the notebooks is to show how the concepts we discussed work in practice.

We will mainly use **scatterplots** and **Sobol sensitivity indices** as two complementary tools.
Scatterplots help us see input‚Äìoutput relationships directly,
while Sobol indices give a quantitative measure of how important each input is for output uncertainty.

We start with a **simple linear model**, as used in the book by Saltelli.
This is the simplest case and helps build intuition about variance and sensitivity.

Then we move to standard **benchmark models**:
the G-function and the Ishigami function.
These models are nonlinear and include strong interaction effects,
so they clearly show why global sensitivity analysis is needed.

Finally, we look at an **applied example**:
a wall model for blood flow simulation.
This shows how the same ideas apply in a realistic engineering context.

In the notebooks, we will compute sensitivity indices using both
**Monte Carlo sampling** and **polynomial chaos expansion (PCE)**.
Both methods estimate the same quantities:
the output variance and the Sobol indices.
The difference is efficiency, not interpretation.

The goal is not to teach software or implementation details.
The goal is to build intuition about uncertainty propagation
and about how sensitivity analysis supports modelling and decision-making.
:::




## Selected references

**Foundations of global sensitivity analysis**
¬†¬†*Global Sensitivity Analysis: The Primer.* Saltelli et al., Wiley, 2008.
¬†¬†*Sensitivity estimates for nonlinear mathematical models.* Sobol‚Äô, I. M., Math. Model. Comput. Exp., 1993.

<br>

**Variance-based methods and practice**
¬†¬†*Variance-based sensitivity analysis of model output: Design and estimator for the total sensitivity index.*
Saltelli et al., Comput. Phys. Commun., 2010.

<br>

**Credibility, uncertainty, and modelling practice**
¬†¬†*Why so many published sensitivity analyses are false.* Saltelli et al., Environ. Model. Softw., 2019.
¬†¬†*Model credibility assessment and uncertainty quantification in in silico trials.* Aldieri et al. (ASME V&V).

<br>

**General overview and terminology**
¬†¬†*Sensitivity analysis.* Wikipedia ‚Äî [https://en.wikipedia.org/wiki/Sensitivity_analysis](https://en.wikipedia.org/wiki/Sensitivity_analysis)


Full reference list:  <a href="../seminar/references.html" target="_blank" rel="noopener">Seminar references </a>
