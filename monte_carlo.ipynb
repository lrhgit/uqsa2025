{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFl-KuVc5lR_"
   },
   "source": [
    "<!-- dom:TITLE: A brief introduction to UQ and SA with the Monte Carlo method -->\n",
    "# A brief introduction to UQ and SA with the Monte Carlo method\n",
    "\n",
    "**Vinzenz Gregor Eck**, Expert Analytics  \n",
    "**Leif Rune Hellevik**, NTNU\n",
    "\n",
    "First version: **Jul 13, 2018**\n",
    "\n",
    "Updated: **Dec 11, 2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sO-dI_ou6h5D",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "205eaccc-0f44-4f73-8fc1-440b575ec37c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chaospy er allerede installert.\n",
      "numpy  : 2.2.6\n",
      "numpoly: 1.3.6\n",
      "chaospy: 4.3.20\n"
     ]
    }
   ],
   "source": [
    "# @title Install chaospy (Colab-friendly)\n",
    "\n",
    "# Denne cellen sørger for at chaospy (og dermed numpoly) er tilgjengelig\n",
    "# uten å tukle med numpy-installasjonen til Colab.\n",
    "\n",
    "try:\n",
    "    import chaospy as cp\n",
    "    import numpoly\n",
    "    import numpy as np\n",
    "    print(\"chaospy er allerede installert.\")\n",
    "except ImportError:\n",
    "    # Installer chaospy fra PyPI. Dette drar inn numpoly automatisk.\n",
    "    %pip install chaospy==4.3.21 --no-cache-dir\n",
    "    import chaospy as cp\n",
    "    import numpoly\n",
    "    import numpy as np\n",
    "\n",
    "print(\"numpy  :\", np.__version__)\n",
    "print(\"numpoly:\", numpoly.__version__)\n",
    "print(\"chaospy:\", cp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/leifh/git/uqsa2025\n",
      "repo_root: /Users/leifh/git/uqsa2025\n",
      "python_source exists: True\n",
      "python_source in sys.path: True\n"
     ]
    }
   ],
   "source": [
    "# @title Repo sync and environment setup\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "REMOTE = \"https://github.com/lrhgit/uqsa2025.git\"\n",
    "REPO_PATH_COLAB = Path(\"/content/uqsa2025\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not REPO_PATH_COLAB.exists():\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", REMOTE, str(REPO_PATH_COLAB)], check=True)\n",
    "    else:\n",
    "        print(\"Updating existing repository...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(REPO_PATH_COLAB), \"pull\"], check=True)\n",
    "    os.chdir(REPO_PATH_COLAB)\n",
    "\n",
    "# --- Find repo root (works locally + in Colab) ---\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = next((p for p in [cwd] + list(cwd.parents) if (p / \".git\").exists()), cwd)\n",
    "\n",
    "PY_SRC = repo_root / \"python_source\"\n",
    "if PY_SRC.exists() and str(PY_SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(PY_SRC))\n",
    "\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"repo_root:\", repo_root)\n",
    "print(\"python_source exists:\", PY_SRC.exists())\n",
    "print(\"python_source in sys.path:\", str(PY_SRC) in sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxOydDQM6oFu",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "dd0b1fb6-a4b8-4278-9662-bd72339b0d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ numpy.reshape patched for numpoly compatibility\n"
     ]
    }
   ],
   "source": [
    "# @title Layout fix, imports, and NumPy compatibility patch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "div.cell.code_cell, div.output {\n",
    "    max-width: 100% !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import chaospy as cp\n",
    "import numpoly\n",
    "import pandas as pd\n",
    "\n",
    "# --- NumPy reshape compatibility patch for numpoly ---\n",
    "_old_reshape = np.reshape\n",
    "def _reshape_compat(a, *args, **kwargs):\n",
    "    newshape = None\n",
    "    if \"newshape\" in kwargs:\n",
    "        newshape = kwargs.pop(\"newshape\")\n",
    "    if \"shape\" in kwargs and newshape is None:\n",
    "        newshape = kwargs.pop(\"shape\")\n",
    "    if newshape is not None:\n",
    "        return _old_reshape(a, newshape, *args, **kwargs)\n",
    "    return _old_reshape(a, *args, **kwargs)\n",
    "np.reshape = _reshape_compat\n",
    "print(\"✓ numpy.reshape patched for numpoly compatibility\")\n",
    "\n",
    "from notebook_utils import pretty_table, section_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oI3gH0Z7bVS",
    "outputId": "5b9edae9-9159-4759-8a9b-e8c22cb9baf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Project modules loaded\n"
     ]
    }
   ],
   "source": [
    "# @title Load project modules\n",
    "\n",
    "from linear_model import linear_model\n",
    "from monte_carlo import generate_sample_matrices_mc, calculate_sensitivity_indices_mc\n",
    "from sensitivity_examples_nonlinear import generate_distributions  # dersom du treng den\n",
    "\n",
    "print(\"✓ Project modules loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_aIybHRH5lSA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot configuration\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "# import seaborn as sns # sets another style\n",
    "matplotlib.rcParams['lines.linewidth'] = 3\n",
    "fig_width, fig_height = (7.0,5.0)\n",
    "matplotlib.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "\n",
    "# font = {'family' : 'sans-serif',\n",
    "#         'weight' : 'normal',\n",
    "#         'size'   : 18.0}\n",
    "# matplotlib.rc('font', **font)  # pass in the font dict as kwar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DkLw4mGh5lSA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chaospy as cp\n",
    "import monte_carlo\n",
    "from sensitivity_examples_nonlinear import generate_distributions\n",
    "from sensitivity_examples_nonlinear import monte_carlo_sens_nonlin\n",
    "from sensitivity_examples_nonlinear import analytic_sensitivity_coefficients\n",
    "from sensitivity_examples_nonlinear import polynomial_chaos_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zUWoPCo5lSA"
   },
   "source": [
    "# Monte Carlo\n",
    "\n",
    "The Monte Carlo method (MCM)  is probably the most widely applied method for\n",
    "variance based uncertainty quantification and sensitivity\n",
    "analysis. Monte carlo methods are generally straight forward to use\n",
    "and may be applied to a wide variety of problems as they require few\n",
    "assumptions about the model or quantity of interest and require no\n",
    "modifications of the model itself, i.e. the model may be used as a\n",
    "black box. The basic idea is to calculate statistics (mean, standard\n",
    "deviation, variance, sobol indices) of $Y$ directly from large amount\n",
    "of sample evaluations from the black box model $y$.\n",
    "\n",
    "\n",
    "\n",
    "<hr/>\n",
    "**Monte Carlo approach.**\n",
    "\n",
    "1. Sample a set of input samples $\\mathbf{z}^{(s)}$ from the input space $\\Omega_\\mathbf{Z}$ that is defined by the joint probability density function ${F_Z}$.\n",
    "\n",
    "2. Evaluate the deterministic model $y(\\mathbf{z})$ for each sample in $\\mathbf{z}^{(s)}$ to produce a set of model outputs $y^{(s)}$.\n",
    "\n",
    "3. Estimate all uncertainty measures and sensitivity indices from $y^{(s)}$.\n",
    "<hr/>\n",
    "\n",
    "\n",
    "\n",
    "For demonstration purposes we will use the same model as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "id": "OgcyVKzV5lSA",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# start the linear model\n",
    "def linear_model(w, z):\n",
    "    return np.sum(w*z, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZOtGHIj5lSA"
   },
   "source": [
    "### Expectation and variance\n",
    "\n",
    "Once the model outputs have been computed the expectation and variance\n",
    "of the output are computed with the normal estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKDViNHU5lSA"
   },
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:expected_value_MonteCarlo\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    {\\mathbb{E}}(Y) \\approx \\frac{1}{N} \\sum_{s=1}^{N} y^{(s)} \\qquad \\text{and} \\qquad       \\operatorname{Var}(Y) \\approx \\frac{1}{N\\!-\\!1} \\sum_{s=1}^{N}  \\left( y^{(s)} - {\\mathbb{E}}(Y)\\right)^2.\n",
    "    \\label{eq:expected_value_MonteCarlo} \\tag{1}\n",
    "  \\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9jmM16u5lSA"
   },
   "source": [
    "Below we demonstrate how  `chaospy` may be used for sampling and `numpy` for the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CF8f0VeAOt3",
    "outputId": "ba4e2445-61a8-4372-ac01-56d4643e8226"
   },
   "outputs": [],
   "source": [
    "# start uq\n",
    "# generate the distributions for the problem\n",
    "Nrv = 4\n",
    "zm = np.array([[0., i] for i in range(1, Nrv + 1)])\n",
    "c = 0.5\n",
    "wm = np.array([[i * c, i] for i in range(1, Nrv + 1)]) \n",
    "jpdf = generate_distributions(zm, wm)\n",
    "\n",
    "# sensitivity analytical values\n",
    "Sa, Szw, Sta = analytic_sensitivity_coefficients(zm, wm)\n",
    "\n",
    "\n",
    "# Monte Carlo\n",
    "#Ns_mc = 1000000 # Number of samples mc\n",
    "Ns_mc = 10000 # Number of samples mc\n",
    "    # calculate sensitivity indices with mc\n",
    "A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "ALeB1mCX5lSA",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a2fb75d9-be28-4de6-ac2e-08527bb42899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E(Y): -0.04420 and  Var(Y): 449.11622\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate a set of Xs\n",
    "Ns = 20000\n",
    "Xs = jpdf.sample(Ns, rule='R').T  # <- transform the sample matrix\n",
    "\n",
    "# 2. Evaluate the model\n",
    "Zs = Xs[:, :Nrv]\n",
    "Ws = Xs[:, Nrv:]\n",
    "Ys = linear_model(Ws, Zs)\n",
    "\n",
    "# 3. Calculate expectation and variance\n",
    "EY = np.mean(Ys)\n",
    "VY = np.var(Ys, ddof=1)  # NB: use ddof=1 for unbiased variance estimator, i.e /(Ns - 1)\n",
    "\n",
    "print('E(Y): {:2.5f} and  Var(Y): {:2.5f}'.format(EY, VY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIivt9qy5lSB"
   },
   "source": [
    "### Variance based sensitivity measures\n",
    "\n",
    "In our [sensitivity_introduction notebook](sensitivity_introduction.ipynb) model we calculated the sensitivity\n",
    "coefficients with the MCM in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (8, 6), indices imply (4, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m row_labels = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, Nrv+\u001b[32m1\u001b[39m)]\n\u001b[32m     23\u001b[39m S = np.column_stack((Sa, Spc, Smc, Sta, Stpc, Stmc))\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSa\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSpc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSmc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStpc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mStmc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_labels\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m display(section_title(\u001b[33m\"\u001b[39m\u001b[33mSensitivity indices (analytic / PCE / MC)\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     32\u001b[39m pretty_table(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/uqsa-py311/lib/python3.11/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/uqsa-py311/lib/python3.11/site-packages/pandas/core/internals/construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/uqsa-py311/lib/python3.11/site-packages/pandas/core/internals/construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (8, 6), indices imply (4, 6)"
     ]
    }
   ],
   "source": [
    "# sensitivity analytical values\n",
    "Sa, Szw, Sta = analytic_sensitivity_coefficients(zm, wm)\n",
    "\n",
    "\n",
    "# Monte Carlo\n",
    "#Ns_mc = 1000000 # Number of samples mc\n",
    "Ns_mc = 1000 # Number of samples mcz'\n",
    "# calculate sensitivity indices with mc\n",
    "A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)\n",
    "\n",
    "# compute with Polynomial Chaos\n",
    "Ns_pc = 40\n",
    "polynomial_order = 2\n",
    "\n",
    "# calculate sensitivity indices with gpc\n",
    "Spc, Stpc, gpce_reg = polynomial_chaos_sens(Ns_pc, jpdf, polynomial_order,return_reg=True)\n",
    "\n",
    "# compare the computations\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "row_labels = [f\"X{i}\" for i in range(1, Nrv+1)]\n",
    "S = np.column_stack((Sa, Spc, Smc, Sta, Stpc, Stmc))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    S,\n",
    "    columns=['Sa','Spc','Smc','Sta','Stpc','Stmc'],\n",
    "    index=row_labels\n",
    ")\n",
    "\n",
    "display(section_title(\"Sensitivity indices (analytic / PCE / MC)\"))\n",
    "pretty_table(df)\n",
    "\n",
    "# Second order indices with gpc\n",
    "\n",
    "S2 = cp.Sens_m2(gpce_reg, jpdf) # second order indices with gpc\n",
    "\n",
    "# print all second order indices\n",
    "df_S2 = pd.DataFrame(S2, columns=row_labels, index=row_labels)\n",
    "\n",
    "display(section_title(\"Second-order Sobol indices\"))\n",
    "pretty_table(df_S2)\n",
    "\n",
    "# sum all second order indices\n",
    "SumS2=np.sum(np.triu(S2))\n",
    "print('\\nSum Sij = {:2.2f}'.format(SumS2))\n",
    "\n",
    "# sum all first and second order indices\n",
    "print('Sum Si + Sij = {:2.2f}\\n'.format(np.sum(Spc)+SumS2))\n",
    "\n",
    "# # compare nonzero second order indices with analytical indices\n",
    "# Szw_pc=[S2[i,i+N_terms] for i in range(N_terms) ]\n",
    "# Szw_table=np.column_stack((Szw_pc,Szw,(Szw_pc-Szw)/Szw))\n",
    "# print(pd.DataFrame(Szw_table,columns=['Szw','Szw pc','Error%']).round(3))\n",
    "\n",
    "# end second order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "collapsed": false,
    "id": "oi9Wx1wa5lSB",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "62ca486b-b438-4e95-d282-e7c66c171738"
   },
   "outputs": [],
   "source": [
    "    # sensitivity analytical values\n",
    "    Sa, Szw, Sta = analytic_sensitivity_coefficients(zm, wm)\n",
    "\n",
    "\n",
    "    # Monte Carlo\n",
    "    #Ns_mc = 1000000 # Number of samples mc\n",
    "    Ns_mc = 10000 # Number of samples mcz'\n",
    "    # calculate sensitivity indices with mc\n",
    "    A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)\n",
    "\n",
    "    # compute with Polynomial Chaos\n",
    "    Ns_pc = 200\n",
    "    polynomial_order = 3\n",
    "\n",
    "    # calculate sensitivity indices with gpc\n",
    "    Spc, Stpc, gpce_reg = polynomial_chaos_sens(Ns_pc, jpdf, polynomial_order,return_reg=True)\n",
    "\n",
    "    # compare the computations\n",
    "    import pandas as pd\n",
    "    row_labels  = ['X_'+str(x) for x in range(1,N_terms*2+1)]\n",
    "    S=np.column_stack((Sa,Spc,Smc,Sta,Stpc,Stmc))\n",
    "    S_table = pd.DataFrame(S, columns=['Sa','Spc','Smc','Sta','Stpc','Stmc'], index=row_labels)\n",
    "    print(S_table.round(3))\n",
    "\n",
    "    # Second order indices with gpc\n",
    "\n",
    "    S2 = cp.Sens_m2(gpce_reg, jpdf) # second order indices with gpc\n",
    "\n",
    "    # print all second order indices\n",
    "    print(pd.DataFrame(S2,columns=row_labels,index=row_labels).round(3))\n",
    "\n",
    "    # sum all second order indices\n",
    "    SumS2=np.sum(np.triu(S2))\n",
    "    print('\\nSum Sij = {:2.2f}'.format(SumS2))\n",
    "\n",
    "    # sum all first and second order indices\n",
    "    print('Sum Si + Sij = {:2.2f}\\n'.format(np.sum(Spc)+SumS2))\n",
    "\n",
    "    # compare nonzero second order indices with analytical indices\n",
    "    Szw_pc=[S2[i,i+N_terms] for i in range(N_terms) ]\n",
    "    Szw_table=np.column_stack((Szw_pc,Szw,(Szw_pc-Szw)/Szw))\n",
    "    print(pd.DataFrame(Szw_table,columns=['Szw','Szw pc','Error%']).round(3))\n",
    "\n",
    "    # end second order\n",
    "\n",
    "    convergence_analysis = False\n",
    "    if convergence_analysis:\n",
    "        # Convergence analysis\n",
    "        # Convergence Monte Carlo with random sampling\n",
    "        list_of_samples = np.array([10000, 50000, 100000, 500000, 1000000])\n",
    "        s_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        st_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        # average over\n",
    "        N_iter = 5\n",
    "        print('MC convergence analysis:')\n",
    "        for i, N_smpl in enumerate(list_of_samples):\n",
    "            print('    N_smpl {}'.format(N_smpl))\n",
    "            for j in range(N_iter):\n",
    "                A_s, XB, XC, Y_A, Y_B, Y_C, S, ST = monte_carlo_sens_nonlin(N_smpl,\n",
    "                                                                                jpdf,\n",
    "                                                                                sample_method='R')\n",
    "                s_mc_err[i] += np.abs(S - Sa)\n",
    "                st_mc_err[i] += np.abs(ST - Sta)\n",
    "                print('         finished with iteration {} of {}'.format(1 + j, N_iter))\n",
    "            s_mc_err[i] /= float(N_iter)\n",
    "            st_mc_err[i] /= float(N_iter)\n",
    "        # Plot results for monte carlo\n",
    "        fig_random = plt.figure('Random sampling - average of indices')\n",
    "        fig_random.suptitle('Random sampling - average of indices')\n",
    "\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        plt.title('First order sensitivity indices')\n",
    "        _=plt.plot(list_of_samples / 1000, np.sum(s_mc_err, axis=1), '-')\n",
    "        ax.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        ax1 = plt.subplot(1, 2, 2)\n",
    "        plt.title('Total sensitivity indices')\n",
    "        _=plt.plot(list_of_samples / 1000, np.sum(st_mc_err, axis=1), '-')\n",
    "        ax1.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        # Plot results for monte carlo figure individual\n",
    "        fig_random = plt.figure('Random sampling')\n",
    "        fig_random.suptitle('Random sampling')\n",
    "        for l, (s_e, st_e) in enumerate(zip(s_mc_err.T, st_mc_err.T)):\n",
    "            ax = plt.subplot(1, 2, 1)\n",
    "            plt.title('First order sensitivity indices')\n",
    "            plt.plot(list_of_samples / 1000, s_e, '-', label='S_{}'.format(l))\n",
    "            ax.set_yscale('log')\n",
    "            _=plt.ylabel('abs error')\n",
    "            _=plt.xlabel('number of samples [1e3]')\n",
    "            _=plt.legend()\n",
    "\n",
    "            ax1 = plt.subplot(1, 2, 2)\n",
    "            plt.title('Total sensitivity indices')\n",
    "            _=plt.plot(list_of_samples / 1000, st_e, '-', label='ST_{}'.format(l))\n",
    "            ax1.set_yscale('log')\n",
    "            _=plt.ylabel('abs error')\n",
    "            _=plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()\n",
    "\n",
    "        # Convergence Polynomial Chaos\n",
    "        list_of_samples = np.array([140, 160, 200, 220])\n",
    "        s_pc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        st_pc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        polynomial_order = 3\n",
    "        # average over\n",
    "        N_iter = 4\n",
    "        print('PC convergence analysis:')\n",
    "        poly = cp.orth_ttr(polynomial_order, jpdf)\n",
    "        for i, N_smpl in enumerate(list_of_samples):\n",
    "            print('    N_smpl {}'.format(N_smpl))\n",
    "            for j in range(N_iter):\n",
    "                # calculate sensitivity indices\n",
    "                Spc, Stpc = polynomial_chaos_sens(N_smpl, jpdf, polynomial_order, poly)\n",
    "                s_pc_err[i] += np.abs(Spc - Sa)\n",
    "                st_pc_err[i] += np.abs(Stpc - Sta)\n",
    "                print('         finished with iteration {} of {}'.format(1 + j, N_iter))\n",
    "            s_pc_err[i] /= float(N_iter)\n",
    "            st_pc_err[i] /= float(N_iter)\n",
    "\n",
    "        # Plot results for polynomial chaos\n",
    "        fig_random = plt.figure('Polynomial Chaos - average of indices')\n",
    "        fig_random.suptitle('Polynomial Chaos - average of indices')\n",
    "\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        plt.title('First order sensitivity indices')\n",
    "        _=plt.plot(list_of_samples, np.sum(s_pc_err, axis=1), '-')\n",
    "        ax.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        ax1 = plt.subplot(1, 2, 2)\n",
    "        plt.title('Total sensitivity indices')\n",
    "        _=plt.plot(list_of_samples, np.sum(st_pc_err, axis=1), '-')\n",
    "        ax1.set_yscale('log')\n",
    "        _=plt.ylabel('abs error')\n",
    "        _=plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "        # Plot results for polynomial chaos individual\n",
    "        fig_random = plt.figure('Polynomial Chaos')\n",
    "        fig_random.suptitle('Polynomial Chaos')\n",
    "        for l, (s_e, st_e) in enumerate(zip(s_pc_err.T, st_pc_err.T)):\n",
    "            ax = plt.subplot(1, 2, 1)\n",
    "            plt.title('First order sensitivity indices')\n",
    "            _=plt.plot(list_of_samples, s_e, '-', label='S_{}'.format(l))\n",
    "            ax.set_yscale('log')\n",
    "            plt.ylabel('abs error')\n",
    "            plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()\n",
    "\n",
    "            ax1 = plt.subplot(1, 2, 2)\n",
    "            plt.title('Total sensitivity indices')\n",
    "            _=plt.plot(list_of_samples, st_e, '-', label='ST_{}'.format(l))\n",
    "            ax1.set_yscale('log')\n",
    "            plt.ylabel('abs error')\n",
    "            plt.xlabel('number of samples [1e3]')\n",
    "            plt.legend()\n",
    "\n",
    "        # # Convergence Monte Carlo with sobol sampling\n",
    "        # list_of_samples = np.array([10000, 50000, 100000, 500000, 1000000])\n",
    "        # s_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        # st_mc_err = np.zeros((len(list_of_samples), N_prms))\n",
    "        # # average over\n",
    "        # N_iter = 10\n",
    "        # for i, N_smpl in enumerate(list_of_samples):\n",
    "        #     for j in range(N_iter):\n",
    "        #         A_s, XB, XC, Y_A, Y_B, Y_C, S, ST = monte_carlo_sens(N_smpl,\n",
    "        #                                                                  jpdf,\n",
    "        #                                                                  sample_method='S')\n",
    "        #         s_mc_err[i] += np.abs(S - Sa)\n",
    "        #         st_mc_err[i] += np.abs(ST - Sta)\n",
    "        #\n",
    "                # print('MC convergence analysis: N_smpl {} - finished with iteration {} of {}'.format(N_smpl, 1 + j, N_iter))\n",
    "        #     s_mc_err[i] /= float(N_iter)\n",
    "        #     st_mc_err[i] /= float(N_iter)\n",
    "        #\n",
    "        # fig_sobol = plt.figure('Sobol sampling')\n",
    "        # fig_sobol.suptitle('Sobol sampling')\n",
    "        # for l, (s_e, st_e) in enumerate(zip(s_mc_err.T, st_mc_err.T)):\n",
    "        #     ax = plt.subplot(1, 2, 1)\n",
    "        #     plt.title('First order sensitivity indices')\n",
    "        #     plt.plot(list_of_samples/1000, s_e, '-', label='S_{}'.format(l))\n",
    "        #     ax.set_yscale('log')\n",
    "        #     plt.ylabel('abs error')\n",
    "        #     plt.xlabel('number of samples [1e3]')\n",
    "        #     plt.legend()\n",
    "        #\n",
    "        #     ax1 = plt.subplot(1, 2, 2)\n",
    "        #     plt.title('Total sensitivity indices')\n",
    "        #     plt.plot(list_of_samples/1000, st_e, '-', label='ST_{}'.format(l))\n",
    "        #     ax1.set_yscale('log')\n",
    "        #     plt.ylabel('abs error')\n",
    "        #     plt.xlabel('number of samples [1e3]')\n",
    "        #     plt.legend()\n",
    "        #\n",
    "        # fig_random = plt.figure('Sobol sampling - average of indices')\n",
    "        # fig_random.suptitle('Sobol sampling - average of indices')\n",
    "        #\n",
    "        # ax = plt.subplot(1, 2, 1)\n",
    "        # plt.title('First order sensitivity indices')\n",
    "        # plt.plot(list_of_samples / 1000, np.sum(s_mc_err, axis=1), '-')\n",
    "        # ax.set_yscale('log')\n",
    "        # plt.ylabel('abs error')\n",
    "        # plt.xlabel('number of samples [1e3]')\n",
    "        #\n",
    "        # ax1 = plt.subplot(1, 2, 2)\n",
    "        # plt.title('Total sensitivity indices')\n",
    "        # plt.plot(list_of_samples / 1000, np.sum(st_mc_err, axis=1), '-')\n",
    "        # ax1.set_yscale('log')\n",
    "        # plt.ylabel('abs error')\n",
    "        # plt.xlabel('number of samples [1e3]')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac1Y9Dau5lSB"
   },
   "source": [
    "The actual algorithm calculating the sensitivity analysis was hidden in this function call which did the magic for us: `A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)`\n",
    "\n",
    "Below we explain in greater detail Saltelli's algorithm which is used to compute the Sobol indices.\n",
    "\n",
    "### Saltelli's algorithm for Sobol indices estimation\n",
    "\n",
    "Calculating the sensitivity coefficients with MCM directly is\n",
    "computationally very expensive. To see this, consider how on would\n",
    "estimate $\\operatorname{Var}\\mathbb{E}(Y|Z_i))$ which is the numerator in the Sobol\n",
    "indices, in a direct brute force, manner. Let $M$ be the evaluations\n",
    "needed to estimate the inner, conditional expected value $\\mathbb{E}(Y|Z_i)$\n",
    "for a fixed $Z_i$. To get an approxiamation of the outer variance, one\n",
    "would have to repeat this process for the whole range of $Z_i$, which\n",
    "could also amount to $\\propto M$. Finally, this would have to be done\n",
    "for all $r$ input random variables of $Y$. Consecquently, the number\n",
    "of evalutations amounts to $\\mathcal{O}(M^2 \\;r)$. To get a impression\n",
    "of what this could to, note that in many cases a reasonable $M$ could\n",
    "be $5000$ which would results in $M^2 =25 000 000$ necessary\n",
    "evaluations!\n",
    "\n",
    "Luckily Saltelli came up with an algorithm to approximate of the sensitivity first order coefficients using $M(p+2)$ evaluations in total\n",
    "There are many adaptations and improvements of the algorithm available, here we will present the basic idea of the algorithm.\n",
    "\n",
    "\n",
    "\n",
    "<hr/>\n",
    "**Saltelli's algorithm.**\n",
    "\n",
    "1. Use a sampling method to draw a set of input samples $\\mathbf{z}^{(s)}$\n",
    "\n",
    "2. Evaluate the deterministic model $y(\\mathbf{z})$ for each sample\n",
    "\n",
    "3. Estimate all sensitivity indices from $y^{(s)}$.\n",
    "<hr/>\n",
    "\n",
    "\n",
    "\n",
    "Thus, the blackbox function mentioned above, follows these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "TlydqK0a5lSB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# calculate sens indices of non additive model\n",
    "def monte_carlo_sens_nonlin(Ns, jpdf, sample_method='R'):\n",
    "\n",
    "    N_prms = len(jpdf)\n",
    "\n",
    "    # 1. Generate sample matrices\n",
    "    XA, XB, XC = generate_sample_matrices_mc(Ns, N_prms, jpdf, sample_method)\n",
    "\n",
    "    # 2. Evaluate the model\n",
    "    Y_A, Y_B, Y_C = evaluate_non_additive_linear_model(XA, XB, XC)\n",
    "\n",
    "    # 3. Approximate the sensitivity indices\n",
    "    S, ST = calculate_sensitivity_indices_mc(Y_A, Y_B, Y_C)\n",
    "\n",
    "    return XA, XB, XC, Y_A, Y_B, Y_C, S, ST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoOva1005lSB"
   },
   "source": [
    "## Saltelli's algorithm step by step\n",
    "### Step 1: sample matrix creation\n",
    "\n",
    "For Saltellis Algorithm we need to create two different sample matrices $A,B$ each of the size $M\\times P$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "642onlHk5lSB"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(A,1)} & \\cdots z_i^{(A,1)} \\cdots & z_P^{(A,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(A,M)} & \\cdots z_i^{(A,M)} \\cdots & z_P^{(A,M)}\n",
    "\\end{bmatrix}\n",
    ", \\quad\n",
    "\\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(B,1)} & \\cdots z_i^{(B,1)} \\cdots & z_P^{(B,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(B,M)} & \\cdots z_i^{(B,M)} \\cdots & z_P^{(B,M)}\n",
    "\\end{bmatrix}\n",
    ".\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Yz0QEqR5lSB"
   },
   "source": [
    "In addition we create $P$ additional matrices $C_i$ of the size $M\\times P$ compound of matrix $A$ and matrix $B$. In a matrix $C_i$ all colums will be have the same values as the $B$ matrix, except the $i$-th column, which will have the values of $A$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcCcZ5K65lSB"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{C}_i =\n",
    "\\begin{bmatrix}\n",
    "z_1^{(B,1)} & \\cdots z_i^{(A,1)} \\cdots & z_P^{(B,1)} \\\\\n",
    "\\vdots &\t\t    & \\vdots \\\\\n",
    "z_i^{(B,M)} & \\cdots z_i^{(A,M)} \\cdots & z_P^{(B,M)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yatx7Jqb5lSB"
   },
   "source": [
    "This was implemented in the method:\n",
    "`A, B, C = generate_sample_matrices_mc(number_of_samples, number_of_parameters, joint_distribution, sample_method)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "7oLG1N3N5lSB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sample matrices\n",
    "def generate_sample_matrices_mc(Ns, number_of_parameters, jpdf, sample_method='R'):\n",
    "\n",
    "    Xtot = jpdf.sample(2*Ns, sample_method).transpose()\n",
    "    A = Xtot[0:Ns, :]\n",
    "    B = Xtot[Ns:, :]\n",
    "\n",
    "    C = np.empty((number_of_parameters, Ns, number_of_parameters))\n",
    "    # create C sample matrices\n",
    "    for i in range(number_of_parameters):\n",
    "        C[i, :, :] = B.copy()\n",
    "        C[i, :, i] = A[:, i].copy()\n",
    "\n",
    "    return A, B, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNK9Z9NB5lSB"
   },
   "source": [
    "### Step 2: evaluate the model for samples\n",
    "\n",
    "In the second step we evaluate the model for samples in the matrices\n",
    "and save the results in vectors $Y_{\\mathbf{A}}$, $Y_{\\mathbf{B}}$ and\n",
    "$Y_{\\mathbf{C_i}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6eNIQR05lSB"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "Y_{\\mathbf{A}} = y(\\mathbf{A}), \\qquad Y_{\\mathbf{B}} = y(\\mathbf{B}), \\qquad  Y_{\\mathbf{C_i}} = y(\\mathbf{C_i}),\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBLxP_2f5lSB"
   },
   "source": [
    "The corresponding python code for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "ao9jizV85lSB",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "def evaluate_non_additive_linear_model(X_A, X_B, X_C):\n",
    "\n",
    "    N_prms = X_A.shape[1]\n",
    "    Ns = X_A.shape[0]\n",
    "    N_terms = int(N_prms / 2)\n",
    "    # 1. evaluate sample matrices X_A\n",
    "    Z_A = X_A[:, :N_terms]  # Split X in two vectors for X and W\n",
    "    W_A = X_A[:, N_terms:]\n",
    "    Y_A = linear_model(W_A, Z_A)\n",
    "\n",
    "    # 2. evaluate sample matrices X_B\n",
    "    Z_B = X_B[:, :N_terms]\n",
    "    W_B = X_B[:, N_terms:]\n",
    "    Y_B = linear_model(W_B, Z_B)\n",
    "\n",
    "    # 3. evaluate sample matrices X_C\n",
    "    Y_C = np.empty((Ns, N_prms))\n",
    "    for i in range(N_prms):\n",
    "        x = X_C[i, :, :]\n",
    "        z = x[:, :N_terms]\n",
    "        w = x[:, N_terms:]\n",
    "        Y_C[:, i] = linear_model(w, z)\n",
    "\n",
    "    return Y_A, Y_B, Y_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VL37HRbf5lSC"
   },
   "source": [
    "### Step 3: approximate the sensitivity indices\n",
    "\n",
    "In the final step the first order and total Sobol indices are estimated.\n",
    "Since the numerical approximation of all indices are quite demanding, approximations are used to speed up the process.\n",
    "For both, the first and total sensitivity index, exist several approximations, which the most common can be found in ([[saltelli2010]](#saltelli2010)).\n",
    "\n",
    "### The first order sensitivity indices\n",
    "\n",
    "The first order indices are defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWLPpops5lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "S_i = \\frac{\\operatorname{Var}\\left(\\mathbb{E}(Y)| Z_i \\right)}{\\operatorname{Var}(Y)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EEDEdbR5lSC"
   },
   "source": [
    "Both, the nominator and denominator are now approximated numerically, whereas the variance (nominator) is defined with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4OGrtzR5lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M \\left(y_{\\mathbf{B}}^j\\right)^2\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO7m2Ufn5lSC"
   },
   "source": [
    "with $f_0^2$ which is $\\left(\\mathbb{E}(Y)\\right)^2$.\n",
    "For $f_0^2$ exist several approximations, two common are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHtynO-e5lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f_0^2 =  \\frac{1}{M^2} \\left(\\sum_{j=1}^M y_{\\mathbf{A}}^j \\right) \\left(  \\sum_{j=1}^M y_{\\mathbf{B}}^j \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOBnCnOt5lSC"
   },
   "source": [
    "The conditional variance is approximated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUZzaJW35lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\mathbb{E}(Y)| Z_i \\right) = \\frac{1}{M-1} \\sum_{j=1}^M y_{\\mathbf{A}}^j y_{\\mathbf{C_i}}^j - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq_m4j_b5lSC"
   },
   "source": [
    "### The total indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eEjzViL5lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "S_{Ti} = \\frac{\\mathbb{E}\\left(\\operatorname{Var}(Y)| \\mathbf{Z}_{-i} \\right)}{\\operatorname{Var}(Y)} = 1 - \\frac{\\operatorname{Var}\\left(\\mathbb{E}(Y)| \\mathbf{Z}_{-i} \\right)}{\\operatorname{Var}(Y)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7LOD5FJ5lSC"
   },
   "source": [
    "Here the variance is estimated accordingly, but taking the matrix A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwNmtv0L5lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}(Y) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M \\left(y_{\\mathbf{A}}^j\\right)^2\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIx7UM-B5lSC"
   },
   "source": [
    "here $f_0^2$ is approximated with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-nCEJ955lSC"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f_0^2 =  \\frac{1}{M^2} \\left(\\sum_{j=1}^M y_{\\mathbf{A}}^j \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PEfQSdZ5lSC"
   },
   "source": [
    "And the conditional variance of not given $Z_i$ is approximated with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1-X906o5lSF"
   },
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\operatorname{Var}\\left(\\mathbb{E}(Y)| \\mathbf{Z}_{-i} \\right) = \\left(\\frac{1}{M-1} \\sum_{j=1}^M y_{\\mathbf{B}}^j y_{\\mathbf{C_i}}^j\\right) - f_0^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJPHBQKt5lSF"
   },
   "source": [
    "Those equations are implemented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "4WCEfEzP5lSF",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# mc algorithm for variance based sensitivity coefficients\n",
    "def calculate_sensitivity_indices_mc(y_a, y_b, y_c):\n",
    "\n",
    "    # single output value y_a for one set of samples\n",
    "    if len(y_c.shape) == 2:\n",
    "        Ns, n_parameters = y_c.shape\n",
    "\n",
    "        # for the first order index\n",
    "        f0sq_first = np.sum(y_a*y_b)/ Ns\n",
    "        y_var_first = np.sum(y_b**2.)/(Ns-1) - f0sq_first\n",
    "\n",
    "        # for the total index\n",
    "        f0sq_total = (sum(y_a)/Ns)**2\n",
    "        y_var_total = np.sum(y_a**2.)/(Ns-1) - f0sq_total\n",
    "\n",
    "        s = np.zeros(n_parameters)\n",
    "        st = np.zeros(n_parameters)\n",
    "\n",
    "        for i in range(n_parameters):\n",
    "            # first order index\n",
    "            cond_var_X = np.sum(y_a*y_c[:, i])/(Ns - 1) - f0sq_first\n",
    "            s[i] = cond_var_X/y_var_first\n",
    "\n",
    "            # total index\n",
    "            cond_exp_not_X = np.sum(y_b*y_c[:, i])/(Ns - 1) - f0sq_total\n",
    "            st[i] = 1 - cond_exp_not_X/y_var_total\n",
    "\n",
    "    # vector output value y_a,.. for one set of samples\n",
    "    elif len(y_c.shape) == 3:\n",
    "        n_y, Ns, n_parameters = y_c.shape\n",
    "        # for the first order index\n",
    "        f0sq_first = np.sum(y_a*y_b, axis=1) / Ns\n",
    "        y_var_first = np.sum(y_b ** 2., axis=1) / (Ns - 1) - f0sq_first\n",
    "\n",
    "        # for the total index\n",
    "        f0sq_total = (np.sum(y_a, axis=1) / Ns) ** 2\n",
    "        y_var_total = np.sum(y_a ** 2., axis=1) / (Ns - 1) - f0sq_total\n",
    "\n",
    "        s = np.zeros((n_parameters, n_y))\n",
    "        st = np.zeros((n_parameters, n_y))\n",
    "\n",
    "        for i in range(n_parameters):\n",
    "            # first order index\n",
    "            cond_var_X = np.sum(y_a * y_c[:, :, i], axis=1) / (Ns - 1) - f0sq_first\n",
    "\n",
    "            s[i, :] = cond_var_X / y_var_first\n",
    "\n",
    "            # total index\n",
    "            cond_exp_not_X = np.sum(y_b * y_c[:, :, i], axis=1) / (Ns - 1) - f0sq_total\n",
    "            st[i, :] = 1 - cond_exp_not_X / y_var_total\n",
    "\n",
    "    return s, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cqjoof_RCp75",
    "outputId": "24c3619d-c933-4469-bd1e-fde2e09e73b8"
   },
   "outputs": [],
   "source": [
    "# Number of samples mc\n",
    "Ns_mc = 10000\n",
    "\n",
    "# calculate sensitivity indices with mc\n",
    "A_s, B_s, C_s, f_A, f_B, f_C, Smc, Stmc = monte_carlo_sens_nonlin(Ns_mc, jpdf)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZodixTg5lSF"
   },
   "source": [
    "# References\n",
    "\n",
    " 1. <div id=\"saltelli2010\"></div> **A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola**.\n",
    "    Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index,\n",
    "    *Computer Physics Communications*,\n",
    "    181(2),\n",
    "    pp. 259-270,\n",
    "    2010."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
